{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the results of models\n",
    "This workbook will walk through an example of model evaluation using Generative AI. Once you have a model working in production, how do you know whether or not the next generation of model is better than the existing one. Also, would a lower cost model provide similar quality results?\n",
    "\n",
    "The example we will review is text summarization. We'll ask multiple models for a summary of a document, then ask a foundation model which provided a better summary.\n",
    "\n",
    "One consideration is that the model doing the comparison should be the most robust. You can have two candidate models be smaller than the comparison model.\n",
    "\n",
    "In this example, we'll use the following candidate models:\n",
    "* **anthropic.claude-instant-v1**: Claude instant\n",
    "* **amazon.titan-tg1-large**: Amazon Titan Large\n",
    "\n",
    "The **anthropic.claude-v2** model will be used for comparing the outputs.\n",
    "\n",
    "## Prerequisites\n",
    "* **Claude** enabled for your AWS Account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create new client\n",
      "  Using region: None\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock(https://bedrock.us-east-1.amazonaws.com)\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils import bedrock, print_ww\n",
    "\n",
    "boto3_bedrock = bedrock.get_bedrock_client(os.environ.get('BEDROCK_ASSUME_ROLE', None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "inference_modifier = {'max_tokens_to_sample':4096, \n",
    "                      \"temperature\":0.5,\n",
    "                      \"top_k\":250,\n",
    "                      \"top_p\":1,\n",
    "                      \"stop_sequences\": [\"\\n\\nHuman\"]\n",
    "                     }\n",
    "\n",
    "textgen_llm = Bedrock(model_id = \"anthropic.claude-v2\",\n",
    "                    client = boto3_bedrock, \n",
    "                    model_kwargs = inference_modifier \n",
    "                    )\n",
    "\n",
    "textgen_claude_instant_llm = Bedrock(model_id = \"anthropic.claude-instant-v1\",\n",
    "                    client = boto3_bedrock, \n",
    "                    model_kwargs = inference_modifier \n",
    "                    )\n",
    "\n",
    "amazon_tital_inference_modifier = {\"maxTokenCount\":512, \n",
    "                      \"temperature\":0,\n",
    "                      \"topP\":1,\n",
    "                      \"stopSequences\": []\n",
    "                     }\n",
    "\n",
    "textgen_amazon_large_llm = Bedrock(model_id = \"amazon.titan-tg1-large\",\n",
    "                    client = boto3_bedrock, \n",
    "                    model_kwargs = amazon_tital_inference_modifier \n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Comparison\n",
    "\n",
    "The notebook will create a summary from an example document, in this case, the Amazon 2022 letter to shareholders. The document is available in [example.py](example.py).\n",
    "\n",
    "It will be run through Claude Instant and Amazon Titan, and the resulting summary will be stored for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from example import letter\n",
    "import json\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "def run_summarization(llm, prompt_template):\n",
    "    multi_var_prompt = PromptTemplate(\n",
    "        input_variables=[\"document_text\"], \n",
    "        template=prompt_template\n",
    "    )\n",
    "\n",
    "    prompt = multi_var_prompt.format(document_text=letter)\n",
    "\n",
    "    response = llm(prompt)\n",
    "    result = response[response.index('\\n')+1:]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Amazon CEO Andy Jassy remains optimistic about the company's future prospects as they navigate\n",
      "current macroeconomic challenges. The company has made adjustments to streamline costs, optimize\n",
      "their fulfillment network, and prioritize investments that have the highest potential to drive long-\n",
      "term value for customers and shareholders. While macroeconomic conditions have softened growth in\n",
      "some areas like AWS, Jassy highlights opportunities in advertising, grocery, Amazon Business, and\n",
      "new initiatives in healthcare and satellite internet that could be transformative if successful.\n",
      "Jassy also discusses the potential of large language models and generative AI to revolutionize\n",
      "machine learning and improve every customer experience at Amazon.\n"
     ]
    }
   ],
   "source": [
    "claude_prompt = \"\"\"Human: You are creating a summary of a document. The original document is encoded by <document></document> tags.\n",
    "<document>{document_text}</document>\n",
    "\n",
    "Please make a 1 paragraph summary of the document.\n",
    "Assistant: \n",
    "\"\"\"\n",
    "\n",
    "claude_instant_summary = run_summarization(textgen_claude_instant_llm, claude_prompt)\n",
    "print_ww(claude_instant_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Despite shutting down some businesses and making changes to others, Jassy remains confident in\n",
      "Amazon's future prospects. He highlights the company's ongoing efforts to improve fulfillment costs,\n",
      "speed up delivery, and expand its retail business. Jassy also discusses Amazon's investments in\n",
      "advertising, machine learning, and new business areas such as healthcare and satellite internet. He\n",
      "emphasizes the company's long-term vision and commitment to innovation.\n"
     ]
    }
   ],
   "source": [
    "amazon_titan_prompt = \"\"\"You are creating a summary of a document.  Please make a 1 paragraph summary of the document.\n",
    "Document:{document_text}\n",
    "\n",
    "Please make a 1 paragraph summary of the document.\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "amazon_titan_summary = run_summarization(textgen_amazon_large_llm, amazon_titan_prompt)\n",
    "print_ww(amazon_titan_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The comparison\n",
    "Now that both summaries have been completed, let's use the Claude v2 model to compare the two and decide which is better.\n",
    "\n",
    "The prompt below provides the entire document text, then both of the summaries. It then asks the foundation model which is more accurate and concise. If you value other dimensions of the summary more, you can include that in the prompt. Lastly, it gives a specific format to the output as an example, thus making it easier to parse.\n",
    "\n",
    "Notice how it includes the reasoning in the result. This tends to drive better results, as the model must justify its decision. Feel free to modify the code to ask for just a Yes/No response to which one is better. The results can be more random in that case.\n",
    "\n",
    "Lastly, run the comparison several times. The answer may change. Is the model hallucinating why one is better than the other? Can more details be added to the prompt to reduce this effect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\n",
      "  \"best\": \"claude_instant\",\n",
      "  \"reason\": \"The claude_instant summary accurately captures the key points from Jassy's letter in a\n",
      "concise manner. It highlights Jassy's optimism despite challenges, adjustments being made, focus on\n",
      "long-term investments, and growth opportunities in new areas like healthcare and AI. The titan\n",
      "summary is also good but includes some extra details that make it slightly less concise.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a prompt template that has multiple input variables\n",
    "multi_var_prompt2 = PromptTemplate(\n",
    "    input_variables=[\"document_text\", \"summary1\", \"summary2\", \"name1\", \"name2\"], \n",
    "    template=\"\"\"Human: You are comparing 2 summaries or a document. The original document is encoded by <document></document> tags. Summary 1 is encoded by <summary1></summary1> and Summary 2 is encoded by <summary2></summary2>\n",
    "\n",
    "<document>{document_text}</document>.\n",
    "\n",
    "<{name1}>{summary1}</{name1}>\n",
    "<{name2}>{summary2}</{name2}>\n",
    "\n",
    "Choosing only between {name1}, {name2}, or uncertain, which summary is more accurate and concise?  Also, explain the reason why with the format:\n",
    "{{\n",
    "  \"best\": \"name1\",\n",
    "  \"reason\": \"The justification for the selection\"\n",
    "}}\n",
    "\n",
    "Only respond with the json format above.\n",
    "Assistant: \n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "prompt = multi_var_prompt2.format(document_text=src_document, summary1=claude_instant_summary, name1=\"claude_instant\", summary2=amazon_titan_summary, name2=\"titan\")\n",
    "\n",
    "\n",
    "response = textgen_llm(prompt)\n",
    "\n",
    "#target_code = response[response.index('\\n')+1:]\n",
    "\n",
    "print_ww(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beat the bot!\n",
    "See if you can beat the bot. Try modifying the summary in the next cell and try to improve the quality of summary. Alternatively put a low quality summary in and see the responses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\n",
      "  \"best\": \"claude-instant\",\n",
      "  \"reason\": \"The claude-instant summary accurately captures the key points from the original\n",
      "document in a concise manner. It highlights Jassy's optimism, the macroeconomic challenges,\n",
      "adjustments being made, focus on high-potential investments, opportunities in new areas like\n",
      "advertising and healthcare, and the potential of AI/ML. The My Summary is very brief and does not\n",
      "provide enough detail on the content.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Replace this summary with your own! Be smarter than Claude\n",
    "your_summary = \"\"\"\n",
    "This is the Amazon annual 2022 shareholder letter written by Andy Jassy. It highlights some of the success\n",
    "and challenges that Amazon has faced and how Amazon plans to respond.\n",
    "\"\"\"\n",
    "\n",
    "prompt = multi_var_prompt2.format(document_text=src_document, summary1=claude_instant_summary, name1=\"claude-instant\", summary2=your_summary, name2=\"My Summary\")\n",
    "\n",
    "\n",
    "response = textgen_llm(prompt)\n",
    "\n",
    "print_ww(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does order matter?\n",
    "\n",
    "Is it just selecting one based on position? Let's flip and re-evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {\n",
      "  \"best\": \"claude-instant\",\n",
      "  \"reason\": \"The claude-instant summary is more accurate and concise. It highlights the key points\n",
      "from the original letter, including Amazon's response to macroeconomic challenges, opportunities in\n",
      "new business areas, and investments in AI/ML. The My Summary is very brief and does not capture the\n",
      "breadth of topics covered.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "prompt = multi_var_prompt2.format(document_text=src_document, summary2=claude_instant_summary, name2=\"claude-instant\", summary1=your_summary, name1=\"My Summary\")\n",
    "\n",
    "\n",
    "response = textgen_llm(prompt)\n",
    "\n",
    "print_ww(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Review - Next Steps\n",
    "This examples a way to use an LLM to evaluate, at scale, the quality of the output of other LLMs. While not perfect, with some repeated testing, you can create an evaluation framework to rate the quality of output of the LLMs to help assess the quality of a model when changing models, either through an upgrade or looking for a lower cost option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
