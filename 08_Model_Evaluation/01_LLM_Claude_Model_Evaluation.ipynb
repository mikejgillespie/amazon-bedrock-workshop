{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the results of models\n",
    "This workbook will walk through an example of model evaluation using Generative AI. Once you have a model working in production, how do you know whether or not the next generation of model is better than the existing one. Also, would a lower cost model provide similar quality results?\n",
    "\n",
    "The example we will review is text summarization. We'll ask multiple models for a summary of a document, then ask a foundation model which provided a better summary.\n",
    "\n",
    "One consideration is that the model doing the comparison should be the most robust. You can have two candidate models be smaller than the comparison model.\n",
    "\n",
    "In this example, we'll use the following candidate models:\n",
    "* **anthropic.claude-instant-v1**: Claude instant\n",
    "* **amazon.titan-tg1-large**: Amazon Titan Large\n",
    "\n",
    "The **anthropic.claude-v2** model will be used for comparing the outputs.\n",
    "\n",
    "## Prerequisites\n",
    "* **Claude** enabled for your AWS Account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create new client\n",
      "  Using region: None\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock(https://bedrock.us-east-1.amazonaws.com)\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils import bedrock, print_ww\n",
    "\n",
    "boto3_bedrock = bedrock.get_bedrock_client(os.environ.get('BEDROCK_ASSUME_ROLE', None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "inference_modifier = {'max_tokens_to_sample':4096, \n",
    "                      \"temperature\":0.5,\n",
    "                      \"top_k\":250,\n",
    "                      \"top_p\":1,\n",
    "                      \"stop_sequences\": [\"\\n\\nHuman\"]\n",
    "                     }\n",
    "\n",
    "textgen_llm = Bedrock(model_id = \"anthropic.claude-v2\",\n",
    "                    client = boto3_bedrock, \n",
    "                    model_kwargs = inference_modifier \n",
    "                    )\n",
    "\n",
    "textgen_claude_instant_llm = Bedrock(model_id = \"anthropic.claude-instant-v1\",\n",
    "                    client = boto3_bedrock, \n",
    "                    model_kwargs = inference_modifier \n",
    "                    )\n",
    "\n",
    "amazon_tital_inference_modifier = {\"maxTokenCount\":512, \n",
    "                      \"temperature\":0,\n",
    "                      \"topP\":1,\n",
    "                      \"stopSequences\": []\n",
    "                     }\n",
    "\n",
    "textgen_amazon_large_llm = Bedrock(model_id = \"amazon.titan-tg1-large\",\n",
    "                    client = boto3_bedrock, \n",
    "                    model_kwargs = amazon_tital_inference_modifier \n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Comparison\n",
    "\n",
    "The notebook will create a summary from an example document, in this case, the Amazon 2022 letter to shareholders. The document is available in [example.py](example.py).\n",
    "\n",
    "It will be run through Claude Instant and Amazon Titan, and the resulting summary will be stored for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from example import letter\n",
    "import json\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "def run_summarization(llm, prompt_template):\n",
    "    multi_var_prompt = PromptTemplate(\n",
    "        input_variables=[\"document_text\"], \n",
    "        template=prompt_template\n",
    "    )\n",
    "\n",
    "    prompt = multi_var_prompt.format(document_text=letter)\n",
    "\n",
    "    response = llm(prompt)\n",
    "    result = response[response.index('\\n')+1:]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Despite facing challenges in 2022 including a difficult macroeconomic environment and operational\n",
      "issues, Amazon was able to grow demand and improve customer experience through investments and\n",
      "adjustments. The company scrutinized its initiatives and made difficult decisions to streamline\n",
      "costs while still prioritizing long-term investments that can drive future growth. Amazon is\n",
      "restructuring its fulfillment network and regionalizing its inventory to improve efficiency and\n",
      "speed. The company sees opportunities for growth in AWS, Advertising, expanding into new retail\n",
      "segments like grocery, and new initiatives in healthcare and satellite internet. Andy Jassy remains\n",
      "optimistic that Amazon will emerge from this challenging time in a stronger position due to its\n",
      "large untapped potential in retail and cloud computing.\n"
     ]
    }
   ],
   "source": [
    "claude_prompt = \"\"\"Human: You are creating a summary of a document. The original document is encoded by <document></document> tags.\n",
    "<document>{document_text}</document>\n",
    "\n",
    "Please make a 1 paragraph summary of the document.\n",
    "Assistant: \n",
    "\"\"\"\n",
    "\n",
    "claude_instant_summary = run_summarization(textgen_claude_instant_llm, claude_prompt)\n",
    "print_ww(claude_instant_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Despite shutting down some businesses and making changes to others, Jassy remains confident in\n",
      "Amazon's future prospects. He highlights the company's ongoing efforts to improve fulfillment costs,\n",
      "speed up delivery, and expand its retail business. Jassy also discusses Amazon's investments in\n",
      "advertising, machine learning, and new business areas such as healthcare and satellite internet. He\n",
      "emphasizes the company's long-term vision and commitment to innovation.\n"
     ]
    }
   ],
   "source": [
    "amazon_titan_prompt = \"\"\"You are creating a summary of a document.  Please make a 1 paragraph summary of the document.\n",
    "Document:{document_text}\n",
    "\n",
    "Please make a 1 paragraph summary of the document.\n",
    "Summary:\n",
    "\"\"\"\n",
    "\n",
    "amazon_titan_summary = run_summarization(textgen_amazon_large_llm, amazon_titan_prompt)\n",
    "print_ww(amazon_titan_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The comparison\n",
    "Now that both summaries have been completed, let's use the Claude v2 model to compare the two and decide which is better.\n",
    "\n",
    "The prompt below provides the entire document text, then both of the summaries. It then asks the foundation model which is more accurate and concise. If you value other dimensions of the summary more, you can include that in the prompt. Lastly, it gives a specific format to the output as an example, thus making it easier to parse.\n",
    "\n",
    "Notice how it includes the reasoning in the result. This tends to drive better results, as the model must justify its decision. Feel free to modify the code to ask for just a Yes/No response to which one is better. The results can be more random in that case.\n",
    "\n",
    "Lastly, run the comparison several times. The answer may change. Is the model hallucinating why one is better than the other? Can more details be added to the prompt to reduce this effect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'src_document' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 22\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create a prompt template that has multiple input variables\u001b[39;00m\n\u001b[1;32m      2\u001b[0m multi_var_prompt2 \u001b[38;5;241m=\u001b[39m PromptTemplate(\n\u001b[1;32m      3\u001b[0m     input_variables\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocument_text\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname2\u001b[39m\u001b[38;5;124m\"\u001b[39m], \n\u001b[1;32m      4\u001b[0m     template\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mHuman: You are comparing 2 summaries or a document. The original document is encoded by <document></document> tags. Summary 1 is encoded by <summary1></summary1> and Summary 2 is encoded by <summary2></summary2>\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     20\u001b[0m )\n\u001b[0;32m---> 22\u001b[0m prompt \u001b[38;5;241m=\u001b[39m multi_var_prompt2\u001b[38;5;241m.\u001b[39mformat(document_text\u001b[38;5;241m=\u001b[39m\u001b[43msrc_document\u001b[49m, summary1\u001b[38;5;241m=\u001b[39mclaude_instant_summary, name1\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclaude_instant\u001b[39m\u001b[38;5;124m\"\u001b[39m, summary2\u001b[38;5;241m=\u001b[39mamazon_titan_summary, name2\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtitan\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m response \u001b[38;5;241m=\u001b[39m textgen_llm(prompt)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m#target_code = response[response.index('\\n')+1:]\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'src_document' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a prompt template that has multiple input variables\n",
    "multi_var_prompt2 = PromptTemplate(\n",
    "    input_variables=[\"document_text\", \"summary1\", \"summary2\", \"name1\", \"name2\"], \n",
    "    template=\"\"\"Human: You are comparing 2 summaries or a document. The original document is encoded by <document></document> tags. Summary 1 is encoded by <summary1></summary1> and Summary 2 is encoded by <summary2></summary2>\n",
    "\n",
    "<document>{document_text}</document>.\n",
    "\n",
    "<{name1}>{summary1}</{name1}>\n",
    "<{name2}>{summary2}</{name2}>\n",
    "\n",
    "Choosing only between {name1}, {name2}, or uncertain, which summary is more accurate and concise?  Also, explain the reason why with the format:\n",
    "{{\n",
    "  \"best\": \"name1\",\n",
    "  \"reason\": \"The justification for the selection\"\n",
    "}}\n",
    "\n",
    "Only respond with the json format above.\n",
    "Assistant: \n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "prompt = multi_var_prompt2.format(document_text=src_document, summary1=claude_instant_summary, name1=\"claude_instant\", summary2=amazon_titan_summary, name2=\"titan\")\n",
    "\n",
    "\n",
    "response = textgen_llm(prompt)\n",
    "\n",
    "#target_code = response[response.index('\\n')+1:]\n",
    "\n",
    "print_ww(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beat the bot!\n",
    "See if you can beat the bot. Try modifying the summary in the next cell and try to improve the quality of summary. Alternatively put a low quality summary in and see the responses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Replace this summary with your own! Be smarter than Claude\n",
    "your_summary = \"\"\"\n",
    "This is the Amazon annual 2022 shareholder letter written by Andy Jassy. It highlights some of the success\n",
    "and challenges that Amazon has faced and how Amazon plans to respond.\n",
    "\"\"\"\n",
    "\n",
    "prompt = multi_var_prompt2.format(document_text=src_document, summary1=claude_instant_summary, name1=\"claude-instant\", summary2=your_summary, name2=\"My Summary\")\n",
    "\n",
    "\n",
    "response = textgen_llm(prompt)\n",
    "\n",
    "print_ww(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does order matter?\n",
    "\n",
    "Is it just selecting one based on position? Let's flip and re-evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = multi_var_prompt2.format(document_text=src_document, summary2=claude_instant_summary, name2=\"claude-instant\", summary1=your_summary, name1=\"My Summary\")\n",
    "\n",
    "\n",
    "response = textgen_llm(prompt)\n",
    "\n",
    "print_ww(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Review - Next Steps\n",
    "This examples a way to use an LLM to evaluate, at scale, the quality of the output of other LLMs. While not perfect, with some repeated testing, you can create an evaluation framework to rate the quality of output of the LLMs to help assess the quality of a model when changing models, either through an upgrade or looking for a lower cost option.\n",
    "\n",
    "To get the most out of this approach, you will need to verify the evaluation prompt returns accurate results. If you alreay have a set of examples that are scored by humans, you can test the LLM's ability to rate the summaries on pre-scored examples. This will provide a baseline to ensure the evaluation of the summaries are accurate. Also, the evaluations can be run multiple times, and provide the temperature setting introduces some variability, you can get a measurement of the number of times the LLM chose one option over X samples. For example, if the evaluation chose Option A 8 out of 10 times, the score would be 80."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
