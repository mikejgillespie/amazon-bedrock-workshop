{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the quality of a model against known data\n",
    "One of the challenges with Generative AI is measuring how effectively a model's output, especially at scale. Due to the stocastic nature of Generative AI, the output can vary from one call to another. Even if directionally the output is the same, you can't do a string comparison of the output, due to the variation in responses.\n",
    "\n",
    "However, there are some things you can do to score the simularity of two responses. \n",
    "\n",
    "### Using GenAI\n",
    "You can ask a model fo compare two responses.\n",
    "\n",
    "### Embeddings\n",
    "One way to compare the output would be to compare the embeddings vector between two responses. \n",
    "\n",
    "## Prerequisites\n",
    "* **Claude** enabled for your AWS Account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create new client\n",
      "  Using region: None\n",
      "boto3 Bedrock client successfully created!\n",
      "bedrock(https://bedrock.us-east-1.amazonaws.com)\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import json\n",
    "import os\n",
    "import sys\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "module_path = \"..\"\n",
    "sys.path.append(os.path.abspath(module_path))\n",
    "from utils import bedrock, print_ww\n",
    "\n",
    "boto3_bedrock = bedrock.get_bedrock_client(os.environ.get('BEDROCK_ASSUME_ROLE', None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.llms.bedrock import Bedrock\n",
    "\n",
    "inference_modifier = {'max_tokens_to_sample':4096, \n",
    "                      \"temperature\":0.5,\n",
    "                      \"top_k\":250,\n",
    "                      \"top_p\":1,\n",
    "                      \"stop_sequences\": [\"\\n\\nHuman\"]\n",
    "                     }\n",
    "\n",
    "textgen_llm = Bedrock(model_id = \"anthropic.claude-v2\",\n",
    "                    client = boto3_bedrock, \n",
    "                    model_kwargs = inference_modifier \n",
    "                    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary Comparison\n",
    "\n",
    "The notebook will create a summary from an example document, in this case, the Amazon 2022 letter to shareholders. The document is available in [example.py](example.py).\n",
    "\n",
    "It will be run through Claude Instant and Amazon Titan, and the resulting summary will be stored for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "baseline_summary = \"\"\"\n",
    "Amazon continues to face challenges but is optimistic about the future. The company has made cost\n",
    "cuts and efficiency improvements in fulfillment and logistics networks to lower costs and speed up\n",
    "delivery times. AWS is facing short-term headwinds but has a strong customer base and continues to\n",
    "innovate with new products and technologies. Other businesses like Advertising, Amazon Business, and\n",
    "Buy with Prime continue to see strong growth. The company is also investing in new areas like\n",
    "grocery, healthcare, satellite internet access, and artificial intelligence which could provide\n",
    "large opportunities in the future. Although Amazon currently has a small share of the overall retail\n",
    "and IT markets, as more retail shifts online and to the cloud, the company believes it is well\n",
    "positioned for significant growth ahead.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Candidate summaries\n",
    "There are four summaries provided.\n",
    "\n",
    "The first was created by Amazon Titan, the second is a completely unrelated article on Honus Wagner, and\n",
    "major leageu baseball player from the turn of the 20th century. The third is a summary of the 2017 letter to shareholders, and the final example is a summary by AI21 labs Jurassic Mid foundation model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "candidates = [\n",
    "{\n",
    "    \"note\": \"Unrelated Article on Honus Wagner\",\n",
    "    \"text\": \"\"\"\n",
    "Johannes Peter \"Honus\" Wagner, sometimes referred to as Hans Wagner, was an American baseball shortstop\n",
    "who played 21 seasons in Major League Baseball from 1897 to 1917, almost entirely for the Pittsburgh Pirates.\n",
    "Wagner won his eighth (and final) batting title in 1911, a National League record that remains unbroken\n",
    "to this day, and matched only once, in 1997, by Tony Gwynn. He also led the league in slugging six times\n",
    "and stolen bases five times. Wagner was nicknamed \"the Flying Dutchman\" due to his superb speed and German\n",
    "heritage. This nickname was a nod to the popular folk-tale made into a famous opera by the German composer\n",
    "Richard Wagner. In 1936, the Baseball Hall of Fame inducted Wagner as one of the first five members.\n",
    "He received the second-highest vote total, behind Ty Cobb's 222 and tied with Babe Ruth at 215.\n",
    "\"\"\"\n",
    "},\n",
    "{\n",
    "    \"note\": \"Anthropic Claude on 2017 Letter\",\n",
    "    \"text\": \"\"\"In his 2017 letter to Amazon shareholders, Jeff Bezos emphasizes the importance \n",
    "of maintaining a \"Day 1\" mentality even as Amazon grows into a large company. He says customer obsession, \n",
    "resisting proxies, embracing external trends, and high-velocity decision making are essential to fending \n",
    "off \"Day 2\" stagnation and irrelevance. Bezos argues that obsessive customer focus drives innovation, that \n",
    "proxies like surveys can mislead, that trends like AI must be quickly embraced, and that quality high-speed \n",
    "decisions maintain energy and dynamism. He shares examples from Amazon like Alexa and Amazon Go to \n",
    "illustrate these points. Bezos stresses that large organizations must move with the spirit of a startup \n",
    "to delight customers and remain vital.\n",
    "\"\"\"\n",
    "},\n",
    "{\n",
    "    \"note\": \"AI21 Labs\",\n",
    "    \"text\": \"\"\"\n",
    "While Amazon has faced a challenging macroeconomic challenges in 2022, it has also been a year of growth\n",
    "and innovation, it has also seen a number of successes. The company continue to grow, innovated, improved \n",
    "its customer experiences, and made important adjustments to its investment strategies. The company has been\n",
    "focusing on long-term investment opportunities, and is constantly investing in long-term opportunities, \n",
    "and plans to lower costs, even as it faces short-term challenges.\n",
    "\"\"\"\n",
    "},\n",
    "{\n",
    "    \"note\": \"Amazon Titan\",\n",
    "    \"text\": \"\"\"\n",
    "Despite shutting down some businesses and making changes to others, Jassy remains confident in\n",
    "Amazon's future prospects. He highlights the company's ongoing efforts to improve fulfillment costs,\n",
    "speed up delivery, and expand its retail business. Jassy also discusses Amazon's investments in\n",
    "advertising, machine learning, and new business areas such as healthcare and satellite internet. He\n",
    "emphasizes the company's long-term vision and commitment to innovation.\n",
    "\"\"\"\n",
    "}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The comparison\n",
    "Now that both summaries have been completed, let's use the Claude v2 model to compare the two and decide which is better.\n",
    "\n",
    "The prompt below provides the entire document text, then both of the summaries. It then asks the foundation model which is more accurate and concise. If you value other dimensions of the summary more, you can include that in the prompt. Lastly, it gives a specific format to the output as an example, thus making it easier to parse.\n",
    "\n",
    "Notice how it includes the reasoning in the result. This tends to drive better results, as the model must justify its decision. Feel free to modify the code to ask for just a Yes/No response to which one is better. The results can be more random in that case.\n",
    "\n",
    "Lastly, run the comparison several times. The answer may change. Is the model hallucinating why one is better than the other? Can more details be added to the prompt to reduce this effect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compareText(text1, text2):\n",
    "    # Create a prompt template that has multiple input variables\n",
    "    multi_var_prompt2 = PromptTemplate(\n",
    "        input_variables=[\"text1\",\"text2\"],\n",
    "        template=\"\"\"Human: You are comparing 2 documents. On a scale of 1 to 100, how similar are the documents\n",
    "\n",
    "    <text1>{text1}</text1>\n",
    "    <text2>{text2}</text2>\n",
    "\n",
    "   Provide the response in JSON format with the similarity score in the score element and explain the justification in the reason element:\n",
    "    {{\n",
    "      \"score\": 100,\n",
    "      \"reason\": \"The justification for the selection\"\n",
    "    }}\n",
    "\n",
    "    Only respond with the json format above.\n",
    "\n",
    "    Assistant: \n",
    "    \"\"\"\n",
    "    )\n",
    "\n",
    "    prompt = multi_var_prompt2.format(text1=text1, text2=text2)\n",
    "\n",
    "\n",
    "    response = textgen_llm(prompt)\n",
    "\n",
    "    #target_code = response[response.index('\\n')+1:]\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unrelated Article on Honus Wagner\n",
      " {\n",
      "  \"score\": 1, \n",
      "  \"reason\": \"The two texts are completely different topics and do not share any similar content, words or themes. The first text is about Amazon's business outlook and the second is a biography of a baseball player. There is no meaningful similarity between them.\"\n",
      "}\n",
      "Anthropic Claude on 2017 Letter\n",
      " {\n",
      "  \"score\": 85, \n",
      "  \"reason\": \"Both texts discuss Amazon's business strategy, challenges, and future outlook. They cover similar topics like AWS, innovation, AI, customer obsession, and maintaining a startup mentality despite growth. There is significant topical overlap between the texts, indicating high similarity, although text2 focuses more narrowly on Jeff Bezos' 2017 letter.\"\n",
      "}\n",
      "AI21 Labs\n",
      " {\n",
      "  \"score\": 95, \n",
      "  \"reason\": \"The two texts are very similar in content and meaning. They both discuss Amazon facing challenges but remaining optimistic and investing for the long-term. The key points around cost cuts, efficiency, AWS, advertising, grocery, healthcare etc. are largely the same.\"\n",
      "}\n",
      "Amazon Titan\n",
      " {\n",
      "  \"score\": 95, \n",
      "  \"reason\": \"The two documents discuss very similar topics and themes related to Amazon's business outlook. They both mention cost cuts, efficiency improvements, growth of AWS, Advertising, and new business areas like healthcare and satellite internet. There is a high degree of semantic overlap between the key points made in each passage.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "for candidate in candidates:\n",
    "    print(candidate['note'])\n",
    "    print(compareText(baseline_summary, candidate['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "See if you can beat the bot. Try modifying the summary in the next cell and try to improve the quality of summary. Alternatively put a low quality summary in and see the responses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.llms.bedrock import Bedrock\n",
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "\n",
    "# - create the Anthropic Model\n",
    "bedrock_embeddings = BedrockEmbeddings(client=boto3_bedrock)\n",
    "\n",
    "hf_evaluator = load_evaluator(\"embedding_distance\",llm = textgen_llm, embeddings=bedrock_embeddings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.17: Fred is a dog that likes to eat steak.->Fred is a cat that likes to eat steak.\n",
      "0.23: Fred is a dog that likes to eat steak.->Fred is a man that likes to eat steak.\n",
      "0.08: Fred is a dog that likes to eat steak.->Fred is a terrier that likes to eat steak.\n",
      "0.15: Fred is a dog that likes to eat steak.->Fred is a dog that likes to eat chicken.\n",
      "0.10: Fred is a dog that likes to eat steak.->Fred is a dog that likes to eat filet mignon.\n"
     ]
    }
   ],
   "source": [
    "def compare_embeddings(prediction, reference):\n",
    "    score = hf_evaluator.evaluate_strings(prediction=prediction, reference=reference)['score']\n",
    "    \n",
    "    print(f\"{score:.2f}: {reference}->{prediction}\")\n",
    "\n",
    "string1 = \"Fred is a dog that likes to eat steak.\"\n",
    "string2 = \"Fred is a cat that likes to eat steak.\"\n",
    "string3 = \"Fred is a man that likes to eat steak.\"\n",
    "string4 = \"Fred is a terrier that likes to eat steak.\"\n",
    "string5 = \"Fred is a dog that likes to eat chicken.\"\n",
    "string6 = \"Fred is a dog that likes to eat filet mignon.\"\n",
    "\n",
    "\n",
    "\n",
    "compare_embeddings(prediction=string2, reference=string1)\n",
    "compare_embeddings(prediction=string3, reference=string1)\n",
    "compare_embeddings(prediction=string4, reference=string1)\n",
    "compare_embeddings(prediction=string5, reference=string1)\n",
    "compare_embeddings(prediction=string6, reference=string1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret Embeddings Results\n",
    "The closer two strings are semantically, the closer they will be to zero. Notice in the example above, that cat is closer to dog from a string distance. But because the embeddings take into account the semantic context of the word, terrier will be closer than dog. Because cats and dogs are both animals, they are closer than man.\n",
    "\n",
    "Again, because filet mignon is a type of steak, the comparison to filet mignon will be closer in meaning than chicken."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unrelated Article on Honus Wagner: 1.0400693128415581\n",
      "Anthropic Claude on 2017 Letter: 0.4652217175377634\n",
      "AI21 Labs: 0.1858441764252151\n",
      "Amazon Titan: 0.22748726407263697\n"
     ]
    }
   ],
   "source": [
    "for candidate in candidates:\n",
    "    score = hf_evaluator.evaluate_strings(prediction=candidate['text'], reference=baseline_summary)['score']\n",
    "    print(f\"{candidate['note']}: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret the results\n",
    "You probably noticed that the results are returned quickly, since the model is only calculating the embeddings, then using the client to compare the embedding vectors. The closer the number is to zero, the more similar the documents are found to be. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Review - Next Steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
